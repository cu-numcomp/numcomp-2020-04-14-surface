{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jax jaxlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit curves and surfaces\n",
    "\n",
    "In this activity, we'll investigate methods for finding the nearest point on an [implicit curve](https://en.wikipedia.org/wiki/Implicit_curve) or implicit surface.\n",
    "Implicit curves are the level sets of functions, and can describe complicated shapes.  For example, the level curve below is the interface between red and blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import jax.numpy as np\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "x1 = np.linspace(-np.pi, np.pi, 30)\n",
    "X, Y = np.meshgrid(x1, x1)\n",
    "F = np.cos(X+Y) - np.cos(X*Y) + .5\n",
    "plt.contourf(X, Y, F, cmap=cm.coolwarm, levels=[-1, -.5, -0.001, 0.001, .5, 1])\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are given a point $\\mathbf x$ and wish to find the closest point $\\mathbf y$ on the level surface; i.e., $f(\\mathbf y) = 0$.\n",
    "This is a constrained optimization problem\n",
    "$$ \\mathbf y_* = \\operatorname{argmin}_{\\mathbf y} \\lVert \\mathbf y - \\mathbf x \\rVert^2, \\quad \\text{subject to}\\, f(\\mathbf y) = 0 . $$\n",
    "\n",
    "We'll use two strategies to solve this problem.  First, we will reformulate the minimization to ask that $\\mathbf g = \\nabla f(\\mathbf y)$ be parallel to $\\mathbf y - \\mathbf x$.\n",
    "\n",
    "Suppose we consider $\\mathbf g$ as a column vector ($2\\times 1$ for an implicit curve; we'll extent to $3\\times 1$ for an implicit surface later) and compute its full QR factorization,\n",
    "\n",
    "$$ \\begin{bmatrix} \\mathbf q_0 & \\mathbf q_1 \\end{bmatrix} \\begin{bmatrix} r_{00} & 0 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} \\mathbf g \\end{bmatrix} . $$\n",
    "Then $\\mathbf q_0$ is parallel to $\\mathbf g$, and $\\mathbf q_1$ is orthogonal.  As such, our minimization problem can be reformulated as solving the equations\n",
    "\n",
    "\\begin{align}\n",
    "  f(\\mathbf y) &= 0 \\\\\n",
    "  \\mathbf q_1^T (\\mathbf y - \\mathbf x) &= 0.\n",
    "\\end{align}\n",
    "\n",
    "Now let's recall that Householder QR expresses the matrix $Q$ as $I - 2 \\mathbf v \\mathbf v^T$ where $\\mathbf v$ is a unit vector.\n",
    "We get $\\mathbf v$ by sending $\\mathbf g$ to $\\lVert \\mathbf g \\rVert \\mathbf e_0$.\n",
    "Here's the code for one column.\n",
    "\n",
    "Note that we are using `jax.numpy`, which doesn't behave exactly like `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def householder1(g):\n",
    "    n = len(g)\n",
    "    v = g + np.sign(g[0]) * np.linalg.norm(g) * np.eye(n,1).flatten()\n",
    "    return v / np.linalg.norm(v)\n",
    "\n",
    "householder1(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class icurve:\n",
    "    def f(self, y):\n",
    "        return np.cos(y[0]+y[1]) - np.cos(y[0]*y[1]) + .5\n",
    "    def grad(self, y):\n",
    "        return jax.grad(self.f)(y)\n",
    "    def hessian(self, y):\n",
    "        return jax.hessian(self.f)(y)\n",
    "\n",
    "ic = icurve()\n",
    "ic.grad(np.array([1., 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.hessian(np.array([1., 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nearest:\n",
    "    def __init__(self, target, x):\n",
    "        self.target = target # Curve or surface\n",
    "        self.x = x           # starting point\n",
    "        \n",
    "    def residual(self, y):\n",
    "        f = self.target.f(y)\n",
    "        g = self.target.grad(y)\n",
    "        v = householder1(g)\n",
    "        d = y - self.x\n",
    "        Qd = d - 2 * v * (v @ d)\n",
    "        return np.hstack([f, Qd[1:]])\n",
    "    \n",
    "#x0 = np.array([1.2, 1.5])\n",
    "x0 = np.array([1.5, 0.])\n",
    "n = nearest(ic, x0)\n",
    "n.residual(np.array([1., 0.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first component of the residual is satisfaction of the surface criteria $f(\\mathbf y) = 0$.\n",
    "* The second is the tangent component.\n",
    "\n",
    "So the above point $\\mathbf y = [1, 0]$ is close to the implicit surface, but the gradient of the implicit function points in a rather different direction from $\\mathbf y - \\mathbf x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(model, x, jit=False):\n",
    "    y = x.copy()\n",
    "    # F and J are callable functions to compute the residual and Jacobian\n",
    "    F = model.residual\n",
    "    J = jax.jacobian(model.residual)\n",
    "    if jit:\n",
    "        F = jax.jit(F)\n",
    "        J = jax.jit(J)\n",
    "    for i in range(10):\n",
    "        resid = F(y)\n",
    "        norm = np.linalg.norm(resid)\n",
    "        print(f'{i} y: {y}')\n",
    "        print(f'{i} residual: {resid} ({norm:.2e})')\n",
    "        if norm < 1e-6:\n",
    "            break\n",
    "        y -= np.linalg.solve(J(y), resid)\n",
    "    return y\n",
    "\n",
    "newton(n, np.array([1., 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convergence is fast, though not monotone (you can experiment with different initial guesses).\n",
    "* Execution speed is slow (there is a perceptable lag between iterations). We'll make that faster by turning on Just In Time (JIT) compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton(n, np.array([1., 0.]), jit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All the same iterates, but executes fast after the first iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate formulation using Lagrange multipliers\n",
    "\n",
    "A powerful technique for handling constraints is to enforce the constraints using Lagrange multipliers.\n",
    "To this end, we write a function\n",
    "\n",
    "$$ L(\\mathbf y, \\lambda) = \\frac 1 2 \\lVert \\mathbf y - \\mathbf x \\rVert^2 + \\lambda f(\\mathbf y) $$\n",
    "\n",
    "and seek a point for which $\\nabla_{\\mathbf y,\\lambda} L = 0$.\n",
    "This point will *not* be a minimum of $L$ (it's a saddle point), but allows us to simultaneously satisfy multiple equations.  In particular, the gradient with respect to $\\lambda$ is the implicit surface equation $f(\\mathbf y) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nearest_lagrange:\n",
    "    def __init__(self, target, x):\n",
    "        self.target = target # Curve or surface\n",
    "        self.x = x           # starting point\n",
    "        \n",
    "    def residual(self, ylam):\n",
    "        y = ylam[:-1]\n",
    "        lam = ylam[-1]\n",
    "        f = self.target.f(y)\n",
    "        g = self.target.grad(y)\n",
    "        return np.hstack([y - self.x + lam * g, f])\n",
    "    \n",
    "nl = nearest_lagrange(ic, x0)\n",
    "newton(nl, np.array([1., 0., 0])) # Now we have to specify an initial guess for lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are now 3 residual components.\n",
    "  * The first two represent satisfaction of $\\mathbf y - \\mathbf x + \\lambda \\mathbf g = 0$ (i.e., that $\\mathbf y - \\mathbf x$ is parallel to $\\mathbf g$ *with* relative length $\\lambda$).\n",
    "  * The last component is satisfaction of $f(\\mathbf y) = 0$ (i.e., that $\\mathbf y$ is on the implicit surface).\n",
    "* Convergence is fast in the terminal phase.\n",
    "* We converge to the same solution (in this case; not guaranteed in case there are multiple solutions).\n",
    "* Convergence is much slower in the initial phase, with $\\lambda$ becoming huge (thereby making the first part of the residual huge) before converging.\n",
    "\n",
    "The erratic initial convergence can be alleviated with line searches and other *globalization* methods, but it represents a common observation that Lagrange multipliers are convenient for modeling (we didn't have to know about Householder QR and differentiate it using `jax`), but are more challenging for optimizers and algebraic solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit Surfaces\n",
    "\n",
    "Write a model for an [implicit surface](https://en.wikipedia.org/wiki/Implicit_surface) and experiment with convergence from different initial guesses.  You could use the [Schwarz P](https://en.wikipedia.org/wiki/Schwarz_minimal_surface#Schwarz_P_(%22Primitive%22)) function, for example.\n",
    "\n",
    "We used the projection technique described in this notebook to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c25e2fb4709bc26befb9f5d40ce511ad",
     "grade": false,
     "grade_id": "cell-26939db61962e43b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class isurface:\n",
    "    def f(self, y):\n",
    "        \"\"\"Return a function of your choice\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    def grad(self, y):\n",
    "        return jax.grad(self.f)(y)\n",
    "    \n",
    "isurf = isurface()\n",
    "x0 = np.array([2., 1, 1])\n",
    "nl = nearest(isurf, x0)\n",
    "newton(nl, x0, jit=True) # Initial guess equals starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c96ebe89280e7a11cf0d77777305b0b6",
     "grade": true,
     "grade_id": "cell-8d26f955537bc8f1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert newton(nl, x0, jit=True)\n",
    "print('Tests pass')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
